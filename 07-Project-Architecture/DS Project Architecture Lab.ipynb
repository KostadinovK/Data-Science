{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from nose.tools import *\n",
    "\n",
    "from scipy.stats import ttest_ind\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# Write your imports here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "525f2882f2b6e191652899e33517abb4",
     "grade": false,
     "grade_id": "cell-1b7f77949e7a3450",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# Data Science Project Architecture Lab\n",
    "## End-to-end project: demonstrating the power of OSEMN. Transition towards modelling. Machine learning basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "On 15 April 1912, the Titanic sank after colliding with an iceberg, killing more than two thirds of the crew and passengers. The dataset provided for you in the `data/` folder contains information about the passengers onboard and which of them survived.\n",
    "\n",
    "The goal of this lab is to explore the data, prepare it for modelling, and perform a (kind of) simple classification. We'll also explore some basics of practical machine learning such as data preparation, testing and training sets, and model evaluation.\n",
    "\n",
    "The original dataset is located [here](https://www.kaggle.com/c/titanic/data). You can read the page for more information about the data and variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1. Read the dataset (1 point)\n",
    "Read the dataset in the `data/titanic.csv` file into the `titanic_data` variable. Here's a short description of what each column means:\n",
    "* PassengerId - a unique number identifying each passenger\n",
    "* Survived - indicator variable: 1 if the passenger survived, 0 otherwise\n",
    "* Pclass - passenger ticket class (1, 2 or 3). This can be used as an indicator of wealth\n",
    "* Name\n",
    "* Sex\n",
    "* Age\n",
    "* SibSp - number of siblings / spouses aboard the Titanic\n",
    "* Parch - number of parents / children aboard the Titanic\n",
    "* Ticket - ticket number\n",
    "* Fare - passenger fare (price)\n",
    "* Cabin - cabin number\n",
    "* Embarked - port of embarkation: C = Cherbourg, Q = Queenstown, S = Southampton\n",
    "\n",
    "**Notes on family relationships:**\n",
    "* Sibling = brother, sister, stepbrother, stepsister\n",
    "* Spouse = husband, wife (mistresses and fianc√©s were ignored)\n",
    "* Parent = mother, father\n",
    "* Child = daughter, son, stepdaughter, stepson. Some children travelled only with a nanny, therefore Parch = 0 for them.\n",
    "\n",
    "Set the index column to be \"PassengerId\". Rename \"Pclass\" to \"Class\" and \"Parch\" to \"ParCh\". Other than that, the column names aren't too bad (although not Pythonic enough). Don't rename them.\n",
    "\n",
    "Also, change the \"Embarked\" column to include the full names of the ports (see the column descriptions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "9f7e61b1a2b4b2484ae37f526f0a863f",
     "grade": false,
     "grade_id": "cell-8d05536388210811",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "titanic_data = None\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "f2593c9c3a6fb7e30c59ff555f621201",
     "grade": true,
     "grade_id": "cell-eeefe71b639dffe8",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert_is_not_none(titanic_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2. Inspect and fill missing data (1 point)\n",
    "See how many records are missing for each column. You can just execute the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that most of the data is there. We have some people with unknown ages and two people with unknown embarkation ports.\n",
    "\n",
    "For missing ages, there are three approaches. We can't say right now which will prove the most correct but we'll stick to one.\n",
    "* Remove people with unknown ages - not desirable, since they are many\n",
    "* Replace unknown ages with a \"centinel\" value, e.g. $-1$ - not desirable because this will introduce invalid data which may throw our models off \n",
    "* Replace unknown ages with the column mean\n",
    "\n",
    "We'll stick with the third approach. Replace the `NaN` values in the `Age` column with the column mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "03704d4a37ab89e1ae20c7b07a8dca02",
     "grade": false,
     "grade_id": "cell-cda0e1f62b8fa17e",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's inspect missing embarkation ports. Store the passengers with unknown embarkation ports in the provided variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "10acfa449e026fa555c709469ad0f7eb",
     "grade": false,
     "grade_id": "cell-c81adf03dbc34dba",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "passengers_with_unknown_embarkation_ports = None\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "passengers_with_unknown_embarkation_ports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see there are two such passengers with the same ticket. We can check there are no other passengers with the same ticket number. We have no idea what to do but we might just replace them with the most common embarkation port.\n",
    "\n",
    "Find out which port was the most common. Replace the two NaN values in the dataset with this port."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "741792adac4959231666ba263fe30166",
     "grade": false,
     "grade_id": "cell-bd2f821dd9cb5fc9",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "most_common_port = None\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "85b021c52a9b2d0f0daa3f112e9f00ff",
     "grade": true,
     "grade_id": "cell-50f02a8a39bf9d82",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Tests for all operations\n",
    "assert_false(titanic_data.Age.isnull().any())\n",
    "\n",
    "assert_is_not_none(passengers_with_unknown_embarkation_ports)\n",
    "assert_is_not_none(most_common_port)\n",
    "assert_false(titanic_data.Embarked.isnull().any())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3. Remove unnecessary columns (1 point)\n",
    "The `Cabin` column contains too many missing values. Probably the best we can do with it is remove it. Also, the names and ticket numbers might be useful in another analysis, but not in this case. We're interested in which passengers survived and we have no reason to think that their names might be related to their survival rate. Also, the ticket numbers are somewhat random.\n",
    "\n",
    "**Note:** It might be interesting to extract the titles of the passengers (e.g. \"Mr.\", \"Miss\", \"Dr.\", etc.) and see whether it correlates to survival rate (e.g. people with higher social status might be more likely to get a boat and survive). But let's not focus on this right now. The class and ticket fare are good enough to indicate social status / wealth.\n",
    "\n",
    "Remove the `Cabin`, `Name`, and `Ticket` columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "10490df105b93dffee0e19d79e3bbe1a",
     "grade": false,
     "grade_id": "cell-4d96142a29f5f032",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "a6743518d8895b2a77bed489ef09cf7f",
     "grade": true,
     "grade_id": "cell-ce236d7fee7f5854",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert_equal(titanic_data.shape, (891, 8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 4. Explore the data: single variables (1 point)\n",
    "Let's start with visualizing single variables. \n",
    "\n",
    "Try plotting a histogram of all ages with 20 bins. You'll see a kind of unusual peak. Remember that this is because we filled in the missing data with the mean of all ages, and it happens to be right where that peak is.\n",
    "\n",
    "Also, try plotting a bar chart (or a pie chart) showing the number of passengers who are male and female. To do this, group the dataset by sex and count the number of rows for each group. `num_passengers_by_sex` should be a `pd.Series` with  two indices: \"male\" and \"female\".\n",
    "\n",
    "Finally, try plotting a histogram of fares to see how asymmetric they are.\n",
    "\n",
    "**Note:** The plots are not autograded, only the data. Feel free to change them, experiment, and add more plots as you see fit. I had quite a lot of fun playing around with different aspects of the data. This is the reason to have EDA, after all :).\n",
    "\n",
    "**Note 2:** The variables should be really simple to set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "ec5f395304a7be79124827f79c7d63ec",
     "grade": false,
     "grade_id": "cell-2c3caaa38c49514a",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "all_ages = None\n",
    "num_passengers_by_sex = None\n",
    "all_fares = None\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "plt.hist(all_ages, bins = 20)\n",
    "plt.title(\"Distribution of ages\")\n",
    "plt.show()\n",
    "\n",
    "plt.gca().set_aspect(\"equal\")\n",
    "plt.pie(num_passengers_by_sex, labels = num_passengers_by_sex.index, autopct = \"%.2f%%\")\n",
    "plt.title(\"Passengers per sex\")\n",
    "plt.show()\n",
    "\n",
    "plt.hist(all_fares, bins = 20)\n",
    "plt.title(\"Distribution of fares\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "99e106190db24b727147d398e2014fdd",
     "grade": true,
     "grade_id": "cell-1e84086c6a0454d7",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert_is_not_none(all_ages)\n",
    "assert_is_not_none(num_passengers_by_sex)\n",
    "assert_is_not_none(all_fares)\n",
    "\n",
    "assert_equal(len(all_ages), len(all_fares))\n",
    "assert_equal(num_passengers_by_sex.index.tolist(), [\"female\", \"male\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 5. Explore correlations in the dataset (1 point)\n",
    "We can play a lot with single variables, groups, etc. But let's focus on correlations now.\n",
    "\n",
    "One of the first things we can do is check all correlations on all variables, like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_data.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, there are some correlations but it seems nothing too interesting can be found.\n",
    "\n",
    "Let's now try some groupings. For example, what percentage of each gender survived? Recall that we calculated the total number of passengers for each gender in the previous exercise.\n",
    "\n",
    "Filter the `titanic_data` dataset to get only survived passengers and apply the same grouping and counting as you did in the previous exercise. You should get a series with \"male\" and \"female\" as the indices.\n",
    "\n",
    "If your answers are correct, the `print()` statements should run without errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "6879abb15cfadda4f2b309d3c4932c25",
     "grade": false,
     "grade_id": "cell-7554388a9c07ce6f",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "survived_passengers = None\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "print(\"Survived men: {} / {}, {:.2f}%\".format(survived_passengers.male, num_passengers_by_sex.male, survived_passengers.male / num_passengers_by_sex.male * 100))\n",
    "print(\"Survived women: {} / {}, {:.2f}%\".format(survived_passengers.female, num_passengers_by_sex.female, survived_passengers.female / num_passengers_by_sex.female * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that a far greater proportion of women survived. This is really significant for two reasons: 1) the difference is really large (74% women vs. 19% men survived), 2) the total number of women on board is smaller.\n",
    "\n",
    "We can therefore conclude that women have been given advantage while evacuating from the ship."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "916a87e6ebac9119d853b000127a150d",
     "grade": true,
     "grade_id": "cell-508e9ba1aadd8279",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert_is_not_none(survived_passengers)\n",
    "assert_equal(num_passengers_by_sex.index.tolist(), [\"female\", \"male\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feel free to look for more correlations if you wish.\n",
    "\n",
    "Let's now focus on something else: the distribution of ages broken down by class. As we already mentioned, passenger class can be used as a proxy for a person's wealth.\n",
    "\n",
    "Group the dataset by class and extract the ages for each group. Store this in the `ages_by_class` variable. It should be a `pd.Series` with `Class` as the index.\n",
    "\n",
    "Plot a histogram showing the three age distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "48fd907f7319418f61746d633edb516a",
     "grade": false,
     "grade_id": "cell-b0f18ef015029cc9",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "ages_by_class = None\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: this is not an autograded cell. It's here only to help you \n",
    "# find out whether your answer and data format are correct\n",
    "assert_is_not_none(ages_by_class)\n",
    "assert_equal(ages_by_class.size().tolist(), [216, 184, 491])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for passenger_class, ages in ages_by_class:\n",
    "    plt.hist(ages, label = \"Class {}\".format(passenger_class), alpha = 0.7)\n",
    "plt.title(\"Distribution of passenger ages per class\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see something really interesting. It seems that first-class passengers are a little bit older than third-class passengers. But is this really true? We can't tell for sure. First of all, there are many more third-class passengers; and second, we can't be sure whether there's a significant difference or not.\n",
    "\n",
    "Fortunately, there's a rigorous statistical method to find out. Enter **hypothesis testing**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 6. Perform hypothesis testing on age vs. class (1 point)\n",
    "First, let's store \"class 1\" and \"class 3\" passenger ages in their own variables, for easier work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_class_ages = ages_by_class.get_group(1)\n",
    "third_class_ages = ages_by_class.get_group(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To perform a hypothesis test, we'll need a hypothesis. Actually, a pair of hypotheses. The \"null hypothesis\", $H_0$ says that \"there's nothing interesting going on with the data\". The \"alternative hypothesis\", $H_1$ says the opposite.\n",
    "\n",
    "We want to prove whether or not the passenger class is correlated with the age. Therefore:\n",
    "* $H_0:$ Passenger class is not correlated with age. `first_class_ages` and `third_class_ages` are two samples from the same distribution.\n",
    "* $H_1:$ `first_class_ages` and `third_class_ages` come from two different distributions.\n",
    "\n",
    "Ideally, **we'd like to reject the null hypothesis**.\n",
    "\n",
    "Here's a quick explanation of the process: we'll perform a test. The exact details aren't important. We assume that $H_0$ is true, therefore **the differences between the two histograms occur simply by chance**. The test will return a $p$-value. It corresponds to the probability that we observe **as extreme or more extreme differences** between the two histograms if $H_0$ is really true.\n",
    "\n",
    "We have to agree on a \"threshold value\" of $p$. Usually that's 5% (0.05), but let's choose 1% in this case. What does this mean? If we reject $H_0$, there will still be 1% chance that we rejected it wrongly.\n",
    "\n",
    "**If $p\\le1\\%$, we will reject $H_0$**.\n",
    "\n",
    "To compare the two variables, it's easiest to perform what's called a **t-test**. It's already been imported for you. Call it like this: `test_result = ttest_ind(<first_variable>, <second_variable>, equal_var = False)`.\n",
    "\n",
    "**Note:** You can get additional information about the mechanics of statistical hypothesis testing on the Internet. Research more if you wish. You can also research what `equal_var = False` is and why we aren't allowed to assume equal variances in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "c82e076dc2c993efb3542dad12387637",
     "grade": false,
     "grade_id": "cell-2fad87583bb70604",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "test_result = None\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "print(test_result.pvalue)\n",
    "if test_result.pvalue <= 0.01:\n",
    "    print(\"The differences in age are significant. Reject H0.\")\n",
    "else:\n",
    "    print(\"There's not enough evidence to reject H0. Don't accept or reject anything else.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "71d34f381017f96f87caee725da0dcd3",
     "grade": true,
     "grade_id": "cell-7ce2e934e8d8ecb9",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert_is_not_none(test_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, we can conclude that **the distributions of ages are significantly different at the 1% level**. Actually, they're so significantly different, that we might as well have chosen $1.10^{-17}\\%$ and still be correct.\n",
    "\n",
    "This means that ages are different for the different classes. How can we interpret this? Probably wealthier people are older. Younger people might not need, or might not be able to afford, a higher fare."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 7. Prepare the data for modelling: indicator variables (1 point)\n",
    "We're going to use `scikit-learn` to model the data. However, that's not so simple. We first need to preprocess the data a little.\n",
    "\n",
    "Most importantly, all variables should be numeric. `scikit-learn` doesn't know how to deal with text and categories.\n",
    "\n",
    "We need to convert `Sex` and `Embarked` to categories. There are many ways to do that.\n",
    "\n",
    "What's considered the best way is via the so-called \"indicator variables\". These are variables whose values are 0 or 1. For example, let's look at the \"Sex\" column. It has two possible values: \"male\" and \"female\". Each of these values will create a new column: `Sex_male` and `Sex_female`. If the passenger is male, he will have 1 in the `Sex_male` column, and so on. Similarly, with `Embarked`.\n",
    "\n",
    "There's a really easy way to do this in `pandas`: `pd.get_dummies(dataframe)`. Note that this returns another dataframe. Add the columns: `[\"Class\", \"Sex\", \"Embarked\"]` to the dataframe. Write the code and explore the newly created dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "579bcc593b53a883fd0f76395615a091",
     "grade": false,
     "grade_id": "cell-ce48a98dc2da3cce",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "titanic_data_for_modelling = None\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "titanic_data_for_modelling.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now see that we have more columns. We can also see that since `Sex` has only two possible values, the two columns `Sex_female` and `Sex_male` are just opposites of each other. We can safely remove one of them. However, this is not true for the `Class` and `Embarked` columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "3f3a74b69ef3afa7e884a482683a1822",
     "grade": false,
     "grade_id": "cell-eb71a6ae99067bb1",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "755132340ca2abe54bf54c8f217dabbd",
     "grade": true,
     "grade_id": "cell-cf11c03fee894a9c",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert_equal(titanic_data_for_modelling.shape, (891, 12))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Later, it will be really convenient to separate the explanatory variables from the target variable.\n",
    "\n",
    "We want to predict whether or not a person has survived. Therefore, `Survived` will be our target variable. All other variables will be our explanatory variables (also called features)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_data_features = titanic_data_for_modelling.drop(\"Survived\", axis = 1)\n",
    "titanic_data_target = titanic_data_for_modelling.Survived"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 8. Prepare the data for modelling: normalization (1 point)\n",
    "In order for the model to perform better, we usually need to rescale the values for each numeric column. \n",
    "\n",
    "Why do we do this? It's related to algorithm stability and convergence. Generally, a machine learning algorithm will perform better if all values are in similar ranges.\n",
    "\n",
    "Do we always need it? No, but we usually do.\n",
    "\n",
    "There are many types of normalization. In this case, we're going to use a **min-max normalization**. The minimum value in the column will become 0, the maximum will become 1. All values in between will be scaled accordingly.\n",
    "\n",
    "`scikit-learn` has a very convenient [MinMaxScaler](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html#sklearn.preprocessing.MinMaxScaler). You use it by simply instantiating it and passing the data:\n",
    "```python\n",
    "scaler = MinMaxScaler()\n",
    "titanic_data_features_scaled = scaler.fit_transform(titanic_data_features)\n",
    "```\n",
    "\n",
    "Note that `titanic_data_scaled` will be a 2D `numpy` array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "da2b97f020a5c62730728fe988fefba8",
     "grade": false,
     "grade_id": "cell-15df658fa2716e19",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "titanic_data_scaled = None\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "183dbb5a7ba39784d3bdc9af8cdddcf2",
     "grade": true,
     "grade_id": "cell-8290e1e8c30f5922",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert_is_not_none(titanic_data_features_scaled)\n",
    "assert_equal(titanic_data_features_scaled.shape, (891, 11))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 9. Split the data (0 points)\n",
    "When we want to evaluate a machine learning model, we usually hide some data from it. We train the model on most of the data, but when we test it afterwards, we pass the additional, hidden data. This is similar to how humans learn - a teacher won't give the exact answers to all students. If this was the case, the teacher cannot know whether a student really learned something, or just memorized all the answers.\n",
    "\n",
    "The function `train_test_split` from `scikit-learn` will perform the splitting for us. See the docs [here](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html).\n",
    "\n",
    "We usually want $\\approx 70\\%$ of the data for training and the remaining $\\approx 30\\%$ for testing. It's very important that the data is shuffled. `train_test_split()` will do this by default.\n",
    "\n",
    "We'll pass the features and target variables and we'll get the different parts accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train, features_test, target_train, target_test = train_test_split(\n",
    "    titanic_data_features_scaled, titanic_data_target, train_size = 0.7, test_size = 0.3, random_state = 42)\n",
    "print(features_train.shape, features_test.shape, target_train.shape, target_test.shape, sep = \"\\r\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 10. Model the data (1 point)\n",
    "Let's model the data using logistic regression. That's very simple.\n",
    "\n",
    "First, create a logistic regression model (with no custom settings). Then, fit the model using the training features and training target.\n",
    "```python\n",
    "model = LogisticRegression()\n",
    "model.fit(???, ???)\n",
    "```\n",
    "\n",
    "If you wish, you can inspect the model coefficients and intercept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "8944f6d44773d414c88c905183f67e8f",
     "grade": false,
     "grade_id": "cell-f0dd6abc403dec0a",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "model = None\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "bb65e5602e097608eaa7ee18a54a8fe7",
     "grade": true,
     "grade_id": "cell-4afb23030f0e02b5",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert_is_not_none(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 11. Score the model (1 point)\n",
    "The default scoring metric for a classification model is **accuracy**. Use `model.score(???, ???)` to get an accuracy score for the model. This should be around 80%.\n",
    "\n",
    "**Note:** Remember to use `features_test` and `target_test`, not the training subsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "c0c14da725bb4e5b5411416a88aca49e",
     "grade": false,
     "grade_id": "cell-528f747f698aeadf",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "score = 0\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "b85e5525d594ea45178885015d5939e5",
     "grade": true,
     "grade_id": "cell-0e64bb27e29b0292",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert_greater(score, 0)\n",
    "assert_less_equal(score, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You see that even though it might seem difficult at first, working with models is pretty easy.\n",
    "\n",
    "Feature preparation, train / test split, normalization, extraction of explanatory features vs. target, modelling, testing, and evaluating: these are all parts of the data modelling process. It's the basic idea of **machine learning**.\n",
    "\n",
    "We started from a dataset and we were able to explore, visualize, and model the data. After all this, we have several deliverables: notebook with our research, model (that we might upload somewhere - but that's outside the scope of this lab), (mostly) repeatable research. We have followed a careful and complete process to get to the final results.\n",
    "\n",
    "We can, of course, extend the study. But this is enough for now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Good luck on the exam! :)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
